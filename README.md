---

# ğŸµ Speech and Sound Signal Processing - Word Segmentation ğŸµ

###  University: University of Piraeus  
###  Department: Informatics  
###  Course: Speech and Sound Signal Processing  

---

## ğŸ¯ Project Overview

In this project, we designed a system that takes a spoken sentence and segments it into words ğŸ—£ï¸âœ‚ï¸.  
The system detects when each word starts and ends, **without knowing in advance how many words** there are â€” only assuming a **small silence gap** between words.

Additionally, we created a program to **play each detected word** separately.  
Finally, the system **estimates the average pitch** (fundamental frequency) of the speaker.

---

## âš™ï¸ Classifiers Implemented

The following classifiers were trained and evaluated:

-  **Least Squares (LSQ)**
-  **Support Vector Machine (SVM)**
-  **Multilayer Perceptron (MLP)** (Three-layer neural network)
-  **Recurrent Neural Network (RNN)**

---

## âš¡ Important Rules

- Programming Language: **Python 3.12.4** ğŸ
- â— No CNNs, no web services, no transfer learning allowed.
-  Deliverables: PDF documentation, source code (source2023.zip), auxiliary files (auxiliary2023.zip).

---

## ğŸ” How It Works

The system does **binary classification**:  
âœ… Speech (foreground) vs âŒ Non-speech (background).

**Main Steps:**
1. Extract **Mel spectrograms** ğŸ¶ from sliding windows of the audio.
2. Classify each window as speech or non-speech.
3. Apply a **median filter** to smooth out small errors ğŸ§¹.
4. Find the **boundaries between words** based on the cleaned-up predictions ğŸ§©.

---

## ğŸ§  Classifier Details

### ğŸ–Šï¸ Least Squares (LSQ) & Support Vector Machine (SVM)
- Simple models that output a continuous value.
- Then we threshold them to get binary speech/non-speech predictions.

### ğŸ§® Multilayer Perceptron (MLP)
- **Two hidden layers**: 128 and 64 neurons with ReLU activation âš¡.
- Output layer: Single neuron with **Sigmoid** activation ğŸ§ .
- Trained with **Binary Cross-Entropy Loss**.

### ğŸ” Recurrent Neural Network (RNN)
- Built using TensorFlow's **SimpleRNN** layers ğŸ”.
- Processes sequences of frames to capture **temporal dynamics**.
- Outputs one probability per time frame.

---

## ğŸ“‚ Datasets

**Foreground (Speech)** ğŸ—£ï¸:  
- Common Voice Corpus Delta Segment 18.0 (as of 6/19/2024) ğŸ“š.

**Background (Noise / Non-speech)** ğŸ”‡:  
- ESC-50 dataset (Harvard Dataverse) ğŸ§.

(Selected only ~150 folders to keep things manageable.)

---

## ğŸ› ï¸ Training

- ğŸ§‘â€ğŸ« **MLP**: trained with Dropout layers to prevent overfitting.
- ğŸ›¡ï¸ **SVM**: trained with LinearSVC from scikit-learn.
- ğŸ”¢ **LSQ**: trained using simple matrix operations.
- ğŸ” **RNN**: trained using SimpleRNN layers to model sequence data.

---

## ğŸ§ª Testing and Evaluation

- ğŸ¯ Tested on three WAV files: 5 seconds, 10 seconds, 20 seconds.
- ğŸ“œ Each test file has:
  - `.txt` file with ground-truth words.
  - `.json` file with ground-truth timestamps.

**Testing Process**:
1. Load the test WAV file.
2. Extract Mel spectrogram features.
3. Predict using all models.
4. Post-process with median filtering.
5. Detect speech segments.
6. Compare predictions to ground-truth annotations ğŸ“.

---

## ğŸ“ Project Structure

| ğŸ“„ File | ğŸ“œ Description |
|:---|:---|
| `train.py` | Training script for all models |
| `test.py` | Testing and evaluation script |

---

## ğŸ›’ Libraries Used

- `os` ğŸ—‚ï¸: File operations
- `numpy` â—: Math operations
- `json` ğŸ“„: Handling annotation files
- `librosa` ğŸ¶: Audio processing
- `joblib` ğŸ’¾: Model saving/loading
- `scikit-learn` ğŸ“š: ML algorithms (SVM, preprocessing)
- `tensorflow.keras` ğŸ¤–: Neural networks (MLP, RNN)

---

## ğŸ”¥ Key Functions

### In `train.py`
- `load_train_audio_clips(limit=None)`: Load training audio.
- `extract_features(audio_clip)`: Get Mel spectrograms.
- `pad_features(features, expected_frames)`: Pad/truncate features.
- Train and save models (MLP, SVM, LSQ, RNN).

### In `test.py`
- Load audio and ground-truth.
- Predict frame-by-frame speech probability.
- Smooth with median filter.
- Detect segments and compare results.

---

## âœ¨ Final Thoughts

This project built a **speech segmentation system** that works **without prior word count knowledge**.  
It uses **traditional machine learning** and **simple RNNs**, without heavy neural network models like CNNs, or any external APIs ğŸŒğŸš«.

---
